{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordcloud(df1, value='', mask_png=None):\n",
    "    # imports\n",
    "    from nltk.corpus import words\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    from spacy.lang.en import English\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from nltk import download\n",
    "    download('wordnet')\n",
    "    from nltk.corpus import stopwords\n",
    "    from wordcloud import WordCloud, ImageColorGenerator\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from plotly.tools import mpl_to_plotly\n",
    "    import base64\n",
    "    \n",
    "    # replace non-alphabetical characters with space using Regex\n",
    "    df1['tweet'] = df1['tweet'].map(lambda x: re.sub(r'[^a-zA-Z] ', ' ', str(x)))\n",
    "    # eliminate rows with empty values for tweet by dropping na's\n",
    "    df1 = df1.dropna(subset=['tweet'])\n",
    "    # stopwords - NLTK\n",
    "    download('stopwords')\n",
    "    # additional stopwords based on previous wordcloud results\n",
    "    more_stopwords = [\"starbucks\", \"want\", \"coffee\", \"like\", \"say\", \"put\", \"nestl\", \"nestle\", \"nestlÃ©\", \"starbuck\",\n",
    "                  \"starbucks\", \"https\", \"cc\", \"co\", \"ht\", \"tps\", \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\",\n",
    "                  \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\",\n",
    "                  \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\",\n",
    "                  \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\",\n",
    "                  \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\",\n",
    "                  \"Mr\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\",\n",
    "                  \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\",\n",
    "                  \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\",\n",
    "                  \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\",\n",
    "                  \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\",\n",
    "                  \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "    stop = stopwords.words('english') + more_stopwords\n",
    "    \n",
    "    # remove encoding of word- strip off any unwanted formatting/http://\n",
    "    def remove_encoding_word(word):\n",
    "        word = str(word)\n",
    "        word = word.encode('ASCII', 'ignore').decode('ASCII')\n",
    "        return word\n",
    "    \n",
    "    # apply remove_encoding_word to each word in text\n",
    "    def remove_encoding_text(text):\n",
    "        text = str(text)\n",
    "        text = ' '.join(remove_encoding_word(word) for word in text.split() if word not in stop)\n",
    "        return text\n",
    "\n",
    "    # apply remove_encoding_word and create lemmatizer\n",
    "    df1['tweet'] = df1['tweet'].apply(remove_encoding_text)\n",
    "    text = ' '.join(words for words in df1['tweet'])\n",
    "    lemma = WordNetLemmatizer().lemmatize\n",
    "\n",
    "    # apply lemmatizer (as opposed to stemming, lemmatizing breaks words down into similar dictionary definitions), filter short words and nonalphabetical characters, and fit TF-IDF model\n",
    "    def tokenize(document):\n",
    "        tokens = [lemma(w) for w in document.split() if len(w) > 3 and w.isalpha()]\n",
    "        return tokens\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(tokenizer=tokenize, ngram_range=((1, 2)),\n",
    "                                 stop_words=stop, strip_accents='unicode')\n",
    "\n",
    "    # fit vectorizer and transform tweets column (safe to ignore warning!)\n",
    "    tdm = vectorizer.fit_transform(df1['tweet'])\n",
    "    # view words\n",
    "    vectorizer.vocabulary_.items()\n",
    "    # calculate TF-IDF weights - fast\n",
    "    n = 1000\n",
    "    items = list(vectorizer.vocabulary_.items())\n",
    "    y = [dict(items[x:x + n + 1]) for x in range(0, len(vectorizer.vocabulary_), n + 1)]\n",
    "    tfidf_weights2 = []\n",
    "    counter = 0\n",
    "    for d in y:\n",
    "        counter += 1\n",
    "        tfidf_weights2.extend([(word, tdm.getcol(idx).sum()) for word, idx in d.items()])\n",
    "        print(\"Processing subdictionary:\", counter, \"of\", len(y))\n",
    "    tfidf_weights2[0:10]\n",
    "\n",
    "    # calculate TF-IDF weights - slow\n",
    "    '''tfidf_weights = [(word, tdm.getcol(idx).sum()) for word, idx in vectorizer.vocabulary_.items()]\n",
    "    len(tfidf_weights)\n",
    "    tfidf_weights[0:10]'''\n",
    "    \n",
    "    # Create Word Cloud\n",
    "    # a) including link to .png file in create_wordcloud command will turn the provided image into a mask for the wordcloud\n",
    "    if mask_png is not None:\n",
    "        twitter_mask2 = np.array(Image.open(mask_png))\n",
    "        image_colors = ImageColorGenerator(twitter_mask2)\n",
    "        w = WordCloud(width=1500, height=1200, mask=twitter_mask2, background_color='white',\n",
    "                    max_words=2000).fit_words(dict(tfidf_weights2))\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        plt.imshow(w.recolor(color_func=image_colors), interpolation=\"bilinear\")\n",
    "        plt.axis('off')\n",
    "        plt.savefig('tweets_wordcloud.png')\n",
    "    # b) not including a .png will still generate a wordcloud, just without a mask\n",
    "    else:\n",
    "        w = WordCloud(width=1500, height=1200, background_color='white',\n",
    "                        max_words=2000).fit_words(dict(tfidf_weights2))\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        plt.imshow(w)\n",
    "        plt.axis('off')\n",
    "        plt.savefig('tweets_wordcloud.png')\n",
    "    \n",
    "    # encode the image into source codes that dash can read\n",
    "    image_filename = 'tweets_wordcloud.png'\n",
    "    encoded_image = base64.b64encode(open(image_filename, 'rb').read())\n",
    "    source='data:image/png;base64,{}'.format(encoded_image.decode())\n",
    "    \n",
    "    return source"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
